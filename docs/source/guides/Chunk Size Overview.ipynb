{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk Size Guide\n",
    "\n",
    "## Overview\n",
    "\n",
    "When Featuretools calculates a feature matrix it is passed or creates a dataframe called `cutoff_time` to determine what data can be used to calculate each row of features.  In this example we create a create a `cutoff_time` dataframe for the \"customers\" entity, with each customer's feature values being calculated at the time of the final purchase by that customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CustomerID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17850.0</th>\n",
       "      <td>17850.0</td>\n",
       "      <td>2011-02-10 14:38:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13047.0</th>\n",
       "      <td>13047.0</td>\n",
       "      <td>2011-11-08 12:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12583.0</th>\n",
       "      <td>12583.0</td>\n",
       "      <td>2011-12-07 08:07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13748.0</th>\n",
       "      <td>13748.0</td>\n",
       "      <td>2011-09-05 09:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15100.0</th>\n",
       "      <td>15100.0</td>\n",
       "      <td>2011-01-13 17:09:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            instance_id                time\n",
       "CustomerID                                 \n",
       "17850.0         17850.0 2011-02-10 14:38:00\n",
       "13047.0         13047.0 2011-11-08 12:10:00\n",
       "12583.0         12583.0 2011-12-07 08:07:00\n",
       "13748.0         13748.0 2011-09-05 09:45:00\n",
       "15100.0         15100.0 2011-01-13 17:09:00"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import featuretools as ft\n",
    "import pandas as pd\n",
    "from featuretools.computational_backends import bin_cutoff_times\n",
    "\n",
    "entityset = ft.demo.load_retail()\n",
    "cutoff_time = pd.DataFrame({'time': entityset[\"customers\"].last_time_index,\n",
    "                            'instance_id': entityset[\"customers\"].df[\"CustomerID\"]})\n",
    "cutoff_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `calculate_feature_matrix` will divide the `cutoff_time` dataframe into smaller `chunks` to calculate on, trying to group rows with the same time in the same chunk.  The cell below shows featuretools creating a 10 row chunk, keeping rows with the same time together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            instance_id                time\n",
      "CustomerID                                 \n",
      "13823.0         13823.0 2011-11-21 10:22:00\n",
      "12658.0         12658.0 2011-11-21 10:22:00\n",
      "14723.0         14723.0 2011-11-29 15:23:00\n",
      "12670.0         12670.0 2011-11-29 15:23:00\n",
      "14824.0         14824.0 2011-11-29 14:20:00\n",
      "13428.0         13428.0 2011-11-29 14:20:00\n",
      "15153.0         15153.0 2011-11-08 17:09:00\n",
      "17022.0         17022.0 2011-11-08 17:09:00\n",
      "15215.0         15215.0 2011-10-26 12:28:00\n",
      "17712.0         17712.0 2011-10-26 12:28:00\n"
     ]
    }
   ],
   "source": [
    "from featuretools.computational_backends import get_next_chunk\n",
    "\n",
    "chunk_iterator = get_next_chunk(cutoff_time=cutoff_time, time_variable='time', num_per_chunk=10)\n",
    "print(chunk_iterator.next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are more rows with the same cutoff time than the chunk size allows, the rows will be be placed in multiple chunks. By binning the timestamp values by 30 minute increments we get times that have more than 10 rows in `cutoff_time`. The chunking algorithm will create a chunk of 10 rows of the same time, then save the other rows to include in another chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rows with timestamp 2011-12-06 12:00\n",
      "\n",
      "            instance_id                time\n",
      "CustomerID                                 \n",
      "17346.0         17346.0 2011-12-06 12:00:00\n",
      "15061.0         15061.0 2011-12-06 12:00:00\n",
      "12971.0         12971.0 2011-12-06 12:00:00\n",
      "16353.0         16353.0 2011-12-06 12:00:00\n",
      "17744.0         17744.0 2011-12-06 12:00:00\n",
      "14735.0         14735.0 2011-12-06 12:00:00\n",
      "14113.0         14113.0 2011-12-06 12:00:00\n",
      "18283.0         18283.0 2011-12-06 12:00:00\n",
      "16979.0         16979.0 2011-12-06 12:00:00\n",
      "14110.0         14110.0 2011-12-06 12:00:00\n",
      "14071.0         14071.0 2011-12-06 12:00:00\n",
      "17250.0         17250.0 2011-12-06 12:00:00\n",
      "16620.0         16620.0 2011-12-06 12:00:00\n",
      "15907.0         15907.0 2011-12-06 12:00:00\n",
      "17481.0         17481.0 2011-12-06 12:00:00\n",
      "13755.0         13755.0 2011-12-06 12:00:00\n",
      "14121.0         14121.0 2011-12-06 12:00:00\n",
      "\n",
      "First chunk\n",
      "\n",
      "            instance_id                time\n",
      "CustomerID                                 \n",
      "17346.0         17346.0 2011-12-06 12:00:00\n",
      "15061.0         15061.0 2011-12-06 12:00:00\n",
      "12971.0         12971.0 2011-12-06 12:00:00\n",
      "16353.0         16353.0 2011-12-06 12:00:00\n",
      "17744.0         17744.0 2011-12-06 12:00:00\n",
      "14735.0         14735.0 2011-12-06 12:00:00\n",
      "14113.0         14113.0 2011-12-06 12:00:00\n",
      "18283.0         18283.0 2011-12-06 12:00:00\n",
      "16979.0         16979.0 2011-12-06 12:00:00\n",
      "14110.0         14110.0 2011-12-06 12:00:00\n",
      "\n",
      "Second chunk\n",
      "\n",
      "            instance_id                time\n",
      "CustomerID                                 \n",
      "18118.0         18118.0 2011-11-29 11:30:00\n",
      "14667.0         14667.0 2011-11-29 11:30:00\n",
      "14866.0         14866.0 2011-11-29 11:30:00\n",
      "17655.0         17655.0 2011-11-29 11:30:00\n",
      "14205.0         14205.0 2011-11-29 11:30:00\n",
      "12727.0         12727.0 2011-11-29 11:30:00\n",
      "15218.0         15218.0 2011-11-29 11:30:00\n",
      "16992.0         16992.0 2011-11-29 11:30:00\n",
      "17711.0         17711.0 2011-11-29 11:30:00\n",
      "14408.0         14408.0 2011-11-29 11:30:00\n"
     ]
    }
   ],
   "source": [
    "cutoff_time_30_minutes = bin_cutoff_times(cutoff_time.copy(), \"30 minutes\")\n",
    "chunk_iterator = get_next_chunk(cutoff_time=cutoff_time_30_minutes, time_variable='time', num_per_chunk=10)\n",
    "\n",
    "print \"All rows with timestamp 2011-12-06 12:00\\n\"\n",
    "print cutoff_time_30_minutes[cutoff_time_30_minutes['time'] == pd.Timestamp(\"2011-12-06 12:00:00\")]\n",
    "print \"\\nFirst chunk\\n\"\n",
    "print(chunk_iterator.next())\n",
    "print \"\\nSecond chunk\\n\"\n",
    "print(chunk_iterator.next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of each chunk is determined by the `chunk_size` parameter  in `dfs` or `calculate_feature_matirx`.  Valid inputs are:\n",
    "\n",
    "* A positive integer (each chunk has this many rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from featuretools.computational_backends import calc_num_per_chunk\n",
    "calc_num_per_chunk(200, cutoff_time.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A float between 0 and 1 (each chunk is a percentage of the entire cutoff dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1093"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each chunk is 25% of all rows in cutoff_time\n",
    "calc_num_per_chunk(0.25, cutoff_time.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* None (the default option. Each chunk will be 10% of the entire cutoff dataframe, or 10 rows per chunk, whichver is bigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_num_per_chunk(None, cutoff_time.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"cutoff time\" \n",
    "\n",
    "Unlike the other options, \"cutoff time\" does not genereate a specific number of rows per chunk.  Instead of trying to create uniformly sized chunks, featuretools will calculate every row with the same time together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cutoff time'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_num_per_chunk(\"cutoff time\", cutoff_time.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the right chunk size can speed up the time spent calculating a feature matrix.  Small chunks can slow down computation in a few ways. Having to split up rows that share the same cutoff time means less shared computation.  If the time necessary to compute a chunk is too short, the cost in overhead for creating that chunk can impact the overall running time of the calculation.  Overly large chunks can slow down computation if the size of the data necessary for the calculations is too large.  What constitutes a good chunk size varies by dataset, cutoff time, and machine hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small number of cutoff times\n",
    "\n",
    "Here we look at the runtimes of several different chunk sizes for a feature matrix calculation with a small number of cutoff times (500) and few cutoff times sharing the same time (1.03 rows per timestamp, on average).\n",
    "\n",
    "#### 10% per chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 134it [00:00, 8343.16it/s]\n",
      "Progress: 100%|██████████| 500/500 [01:19<00:00,  6.23cutoff time/s]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix, features = ft.dfs(entityset=entityset,\n",
    "                                  target_entity=\"customers\",\n",
    "                                  verbose=True,\n",
    "                                  cutoff_time=cutoff_time.iloc[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 134it [00:00, 6051.02it/s]\n",
      "Progress: 100%|██████████| 500/500 [01:21<00:00,  6.11cutoff time/s]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix, features = ft.dfs(entityset=entityset,\n",
    "                                  target_entity=\"customers\",\n",
    "                                  verbose=True,\n",
    "                                  cutoff_time=cutoff_time.iloc[:500],\n",
    "                                  chunk_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 rows per chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 134it [00:00, 4867.93it/s]\n",
      "Progress: 100%|██████████| 500/500 [01:45<00:00,  4.61cutoff time/s]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix, features = ft.dfs(entityset=entityset,\n",
    "                                  target_entity=\"customers\",\n",
    "                                  verbose=True,\n",
    "                                  cutoff_time=cutoff_time.iloc[:500],\n",
    "                                  chunk_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"cutoff time\" option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 134it [00:00, 5268.34it/s]\n",
      "Progress: 100%|██████████| 500/500 [01:44<00:00,  3.66cutoff time/s]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix, features = ft.dfs(entityset=entityset,\n",
    "                                  target_entity=\"customers\",\n",
    "                                  verbose=True,\n",
    "                                  cutoff_time=cutoff_time.iloc[:500],\n",
    "                                  chunk_size=\"cutoff time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of cutoff times is so small and there are few cutoff times with the same timestamp, calculating all cutoff times in the same chunk is comparable to the default 10% approach.  With a very small chunk size of 2 rows per chunk, the calculation has slowed down considerably.  Grouping by time is similarly slow, which seems reasonable given the avergae number of rows per cutoff time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All cutoff times\n",
    "\n",
    "Using all of the cutoff times does not change how few rows have the same cutoff time.  The default chunk size of 10% and calculating all of the cutoff times in a single chunk still takes about the same amount of time. Using a chunk size of 2 is noticably slower, and grouping by cutoff time even more so.\n",
    "\n",
    "#### 10% per chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 134it [00:00, 7222.17it/s]\n",
      "Progress: 100%|██████████| 4373/4373 [10:22<00:00,  6.60cutoff time/s]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix, features = ft.dfs(entityset=entityset,\n",
    "                                  target_entity=\"customers\",\n",
    "                                  verbose=True,\n",
    "                                  cutoff_time=cutoff_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 134it [00:00, 6128.48it/s]\n",
      "Progress: 100%|██████████| 4373/4373 [10:25<00:00,  6.99cutoff time/s]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix, features = ft.dfs(entityset=entityset,\n",
    "                                  target_entity=\"customers\",\n",
    "                                  verbose=True,\n",
    "                                  cutoff_time=cutoff_time,\n",
    "                                  chunk_size=4373)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 rows per chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 134it [00:00, 5178.11it/s]\n",
      "Progress: 100%|██████████| 4373/4373 [14:03<00:00,  4.00cutoff time/s]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix, features = ft.dfs(entityset=entityset,\n",
    "                                  target_entity=\"customers\",\n",
    "                                  verbose=True,\n",
    "                                  cutoff_time=cutoff_time,\n",
    "                                  chunk_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"cutoff time\" option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 134it [00:00, 5697.74it/s]\n",
      "Progress: 100%|██████████| 4373/4373 [19:18<00:00,  2.61cutoff time/s]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix, features = ft.dfs(entityset=entityset,\n",
    "                                  target_entity=\"customers\",\n",
    "                                  verbose=True,\n",
    "                                  cutoff_time=cutoff_time,\n",
    "                                  chunk_size=\"cutoff time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fewer Unique Cutoff Times\n",
    "\n",
    "Next we group the timestamps in the cutoff time dataframe by week to reduce the number of unique cutoff times to 55 from over 4000.  This speeds up the overall computation because there is more shareable computation when calculating.  This also causes a more noticeable drop in performance when choosing chunk sizes much smaller than the number of unique cutoff times per instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique cutoff times: 55\n",
      "Largest cutoff group: 532 instances\n",
      "Largest cutoff percent of whole: 0.121656\n"
     ]
    }
   ],
   "source": [
    "binned_cutoffs = bin_cutoff_times(cutoff_time.copy(), '7 days')\n",
    "grouped = binned_cutoffs.groupby('time').count()\n",
    "print(\"Number of unique cutoff times: \" + str(len(grouped)))\n",
    "max_cutoff_size = grouped.max()[0]\n",
    "print(\"Largest cutoff group: %d instances\" % max_cutoff_size)\n",
    "max_cutoff_percent = float(max_cutoff_size)/binned_cutoffs.shape[0]\n",
    "print(\"Largest cutoff percent of whole: %f\" % (max_cutoff_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CustomerID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17850.0</th>\n",
       "      <td>17850.0</td>\n",
       "      <td>2011-02-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13047.0</th>\n",
       "      <td>13047.0</td>\n",
       "      <td>2011-11-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12583.0</th>\n",
       "      <td>12583.0</td>\n",
       "      <td>2011-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13748.0</th>\n",
       "      <td>13748.0</td>\n",
       "      <td>2011-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15100.0</th>\n",
       "      <td>15100.0</td>\n",
       "      <td>2011-01-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            instance_id       time\n",
       "CustomerID                        \n",
       "17850.0         17850.0 2011-02-10\n",
       "13047.0         13047.0 2011-11-03\n",
       "12583.0         12583.0 2011-12-01\n",
       "13748.0         13748.0 2011-09-01\n",
       "15100.0         15100.0 2011-01-13"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binned_cutoffs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10% per chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 134it [00:00, 4504.37it/s]\n",
      "Progress: 100%|██████████| 4373/4373 [00:54<00:00, 95.12cutoff time/s]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix, features = ft.dfs(entityset=entityset,\n",
    "                                  cutoff_time=binned_cutoffs,\n",
    "                                  target_entity=\"customers\",\n",
    "                                  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 134it [00:00, 7367.11it/s]\n",
      "Progress: 100%|██████████| 4373/4373 [00:53<00:00, 82.29cutoff time/s]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix, features = ft.dfs(entityset=entityset,\n",
    "                                  cutoff_time=binned_cutoffs,\n",
    "                                  target_entity=\"customers\",\n",
    "                                  chunk_size=4373,\n",
    "                                  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean cutoff time group size\n",
    "\n",
    "Trying a new chunk size, the average cutoff time group size, or about 1.829% of the entire list of cutoff times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instance_id    79.490909\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binned_cutoffs.groupby('time').count().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 134it [00:00, 5199.23it/s]\n",
      "Progress: 100%|██████████| 4373/4373 [00:57<00:00, 93.67cutoff time/s] \n"
     ]
    }
   ],
   "source": [
    "feature_matrix, features = ft.dfs(entityset=entityset,\n",
    "                                  cutoff_time=binned_cutoffs,\n",
    "                                  target_entity=\"customers\",\n",
    "                                  chunk_size=80,\n",
    "                                  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 cutoffs per chunk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 134it [00:00, 7815.29it/s]\n",
      "Progress: 100%|██████████| 4373/4373 [03:52<00:00, 28.70cutoff time/s]\n"
     ]
    }
   ],
   "source": [
    "# Defult chunk size of 10%\n",
    "feature_matrix, features = ft.dfs(entityset=entityset,\n",
    "                                  cutoff_time=binned_cutoffs,\n",
    "                                  target_entity=\"customers\",\n",
    "                                  chunk_size=10,\n",
    "                                  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"cutoff time\" option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 134it [00:00, 6469.41it/s]\n",
      "Progress: 100%|██████████| 4373/4373 [00:55<00:00, 47.60cutoff time/s] \n"
     ]
    }
   ],
   "source": [
    "feature_matrix, features = ft.dfs(entityset=entityset,\n",
    "                                  cutoff_time=binned_cutoffs,\n",
    "                                  target_entity=\"customers\",\n",
    "                                  chunk_size=\"cutoff time\",\n",
    "                                  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instacart Dataset\n",
    "\n",
    "Next we'll try some examples using the Instacart dataset used in the Predict Next Purchase demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            user_id  label\n",
      "time                      \n",
      "2015-03-15     7790   7790\n",
      "2015-06-15     4458   4458\n",
      "2015-09-15     2515   2515\n",
      "2015-12-15     1091   1091\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "# import entityset\n",
    "es = utils.load_entityset(\"partitioned_data/part_1/\")\n",
    "\n",
    "# make labels and cutoff times\n",
    "label_times = pd.concat([utils.make_labels(es=es,\n",
    "                                           product_name = \"Banana\",\n",
    "                                           cutoff_time = pd.Timestamp('March 15, 2015'),\n",
    "                                           prediction_window = ft.Timedelta(\"4 weeks\"),\n",
    "                                           training_window = ft.Timedelta(\"60 days\")),\n",
    "                         utils.make_labels(es=es,\n",
    "                                           product_name = \"Banana\",\n",
    "                                           cutoff_time = pd.Timestamp('June 15, 2015'),\n",
    "                                           prediction_window = ft.Timedelta(\"4 weeks\"),\n",
    "                                           training_window = ft.Timedelta(\"60 days\")),\n",
    "                         utils.make_labels(es=es,\n",
    "                                           product_name = \"Banana\",\n",
    "                                           cutoff_time = pd.Timestamp('September 15, 2015'),\n",
    "                                           prediction_window = ft.Timedelta(\"4 weeks\"),\n",
    "                                           training_window = ft.Timedelta(\"60 days\")),\n",
    "                         utils.make_labels(es=es,\n",
    "                                           product_name = \"Banana\",\n",
    "                                           cutoff_time = pd.Timestamp('December 15, 2015'),\n",
    "                                           prediction_window = ft.Timedelta(\"4 weeks\"),\n",
    "                                           training_window = ft.Timedelta(\"60 days\")),],\n",
    "                       ignore_index=True)\n",
    "\n",
    "print(label_times.groupby('time').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One cutoff time group accounts for nearly half of all rows of the cutoff time dataframe. As we can see by comparing the single chunk and group by cutoff time cases with the default 10% chunk size, in this scenario keeping the entire cutoff time group together slows down the computation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10% per chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 120it [00:00, 3605.88it/s]\n",
      "Progress: 100%|██████████| 15854/15854 [03:21<00:00, 71.49cutoff time/s]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix, features = ft.dfs(target_entity=\"users\", \n",
    "                                  cutoff_time=label_times,\n",
    "                                  training_window=ft.Timedelta(\"60 days\"),\n",
    "                                  entityset=es,\n",
    "                                  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 120it [00:00, 3105.82it/s]\n",
      "Progress: 100%|██████████| 15854/15854 [04:26<00:00, 59.55cutoff time/s]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix, features = ft.dfs(target_entity=\"users\", \n",
    "                                  cutoff_time=label_times,\n",
    "                                  training_window=ft.Timedelta(\"60 days\"),\n",
    "                                  entityset=es,\n",
    "                                  chunk_size=15854,\n",
    "                                  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 250 per chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 120it [00:00, 2267.82it/s]\n",
      "Progress: 100%|██████████| 15854/15854 [03:14<00:00, 64.78cutoff time/s]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix, features = ft.dfs(target_entity=\"users\", \n",
    "                                  cutoff_time=label_times,\n",
    "                                  training_window=ft.Timedelta(\"60 days\"),\n",
    "                                  entityset=es,\n",
    "                                  chunk_size=250,\n",
    "                                  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 per chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 120it [00:00, 3566.43it/s]\n",
      "Progress: 100%|██████████| 15854/15854 [03:41<00:00, 56.09cutoff time/s]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix, features = ft.dfs(target_entity=\"users\", \n",
    "                                  cutoff_time=label_times,\n",
    "                                  training_window=ft.Timedelta(\"60 days\"),\n",
    "                                  entityset=es,\n",
    "                                  chunk_size=100,\n",
    "                                  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"cutoff time\" option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 120it [00:00, 2912.12it/s]\n",
      "Progress: 100%|██████████| 15854/15854 [04:22<00:00, 60.86cutoff time/s]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix, features = ft.dfs(target_entity=\"users\", \n",
    "                                  cutoff_time=label_times,\n",
    "                                  training_window=ft.Timedelta(\"60 days\"),\n",
    "                                  entityset=es,\n",
    "                                  chunk_size=\"cutoff time\",\n",
    "                                  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary \n",
    "\n",
    "In review, featuretools uses a parameter `chunk_size` to divide up the instances when calculating features.  Creating chunks that have too little or too much calculation can slow down the calculations, so experimenting with different chunk sizes on a subset of the data before calculating on the entire can help find in finding a reasonable chunk size.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
